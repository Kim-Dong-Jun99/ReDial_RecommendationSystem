{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj5cZRjuICL6"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpyzQZ-7ICMD"
      },
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset imports\n",
        "import json\n",
        "\n",
        "# For restoring the dataset\n",
        "from copy import deepcopy\n",
        "\n",
        "# Text manipulations\n",
        "import re\n",
        "\n",
        "# TF-IDF\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Cosine similarity\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# File download (Golve)\n",
        "import os.path\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmVkjfdUIKAd",
        "outputId": "5353830c-7db5-4aad-8538-211943d3322d"
      },
      "outputs": [],
      "source": [
        "DRIVE_PATH = '/content/drive'\n",
        "BASE_PATH = '/MyDrive/MachineLearningTP/'\n",
        "GLOVE_PATH = 'glove/'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(DRIVE_PATH)\n",
        "BASE_PATH = DRIVE_PATH + BASE_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Download File: GloVe6B.zip - glove100d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filepath = BASE_PATH + GLOVE_PATH + 'glove.6B.100d.txt'\n",
        "zippath = BASE_PATH + GLOVE_PATH + 'glove.6B.zip'\n",
        "\n",
        "if os.path.exists(filepath) is False:\n",
        "    if os.path.exists(BASE_PATH + GLOVE_PATH) is False:\n",
        "        os.mkdir(BASE_PATH + GLOVE_PATH)\n",
        "    \n",
        "    if os.path.exists(zippath) is False:\n",
        "        urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=zippath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all\n",
        "if os.path.exists(filepath) is False:\n",
        "    zf = zipfile.ZipFile(zippath)\n",
        "    zf.extractall(BASE_PATH + GLOVE_PATH) \n",
        "    zf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1aJVCm4ICME"
      },
      "source": [
        "# Library installation\n",
        "* NLTK - Natural Language toolkit\n",
        "* NetworkX - Structure, Dynamics, and Functions of complex networks Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4pSnYQIICMF",
        "outputId": "463b4c6e-c361-4c93-89d0-25d682c78ae7"
      },
      "outputs": [],
      "source": [
        "!python -m pip install nltk\n",
        "!python -m pip install networkx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psqtmrcmICMG"
      },
      "source": [
        "NLTK: Library for NLP Process\n",
        "* Usage:\n",
        "    * nltk.corpus.**stopwords**: stopwords of specific language\n",
        "    * nltk.tokenize.**RegexpTokenizer, sent_tokenize, word_tokenize**: Tokenize the input sentences\n",
        "    * nltk.stem.**WordNetLemmatizer**: Lemmatize the word net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ybu3JyZICMG",
        "outputId": "22922dfb-cdb2-4087-a9fb-ddd5ab31d0a0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ2AlrSRICMH"
      },
      "source": [
        "# Redial Parser\n",
        "A separated library for parsing the redial dataset\n",
        "\n",
        "class **RedialParser**\n",
        "- Restore(): Restore train, test, and movie dataset to initial state\n",
        "   * return:\n",
        "        * None\n",
        "- Movies(train): Get movie list in dataset\n",
        "   * param:\n",
        "        * train (bool): Target dataset, (train=True, test=False, all=None)\n",
        "   * return:\n",
        "        * dict: {index, MovieName}\n",
        "- describe(): Describe its datasets\n",
        "   * return:\n",
        "        * None\n",
        "- train: Train data of ReDial.\n",
        "- test: Test data of ReDial.\n",
        "- movie: Movie mention counts for ReDial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsGx6WIzICMH"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    TODO: initialization function for dataset reads\n",
        "\n",
        "        :arg\n",
        "            path (str): Dataset path.\n",
        "        :return\n",
        "            tuple: (train, test, df_mention)\n",
        "    \"\"\"\n",
        "    train_data = []\n",
        "    for line in open(f\"{path}/train_data.jsonl\", \"r\"):\n",
        "        train_data.append(json.loads(line))\n",
        "\n",
        "    test_data = []\n",
        "    for line in open(f\"{path}/test_data.jsonl\", \"r\"):\n",
        "        test_data.append(json.loads(line))\n",
        "\n",
        "    mention_dataframe = pd.read_csv(f\"{path}/movies_with_mentions.csv\")\n",
        "\n",
        "    return train_data, test_data, mention_dataframe\n",
        "\n",
        "\n",
        "\n",
        "def predict_rating(grouped_arr, word_sim_arr):\n",
        "    ratings_pred = grouped_arr.dot(word_sim_arr) / np.array([np.abs(word_sim_arr).sum(axis=1)])\n",
        "    return ratings_pred\n",
        "\n",
        "\n",
        "class RedialParser:\n",
        "    def __init__(self, path):\n",
        "        self.train, self.test, self.movie = load_data(path)\n",
        "\n",
        "        self._global_movie_list = None  # list of all movie names (global movie name data)\n",
        "        self._global_msg_list = None  # list of whole lines (global line data)\n",
        "        self._local_movie_list = None  # list of movie names (local movie name data)\n",
        "        self._local_msg_list = None  # list of lines (local line data)\n",
        "\n",
        "        self.dialog_df = None  # Sum of dialogs for each movie indices\n",
        "\n",
        "        self.__train = deepcopy(self.train)\n",
        "        self.__test = deepcopy(self.test)\n",
        "        self.__movie = deepcopy(self.movie)\n",
        "\n",
        "        # Import 100-D GloVe Embedding Vector\n",
        "        self.__glove_dict = dict()\n",
        "        f = open(BASE_PATH + GLOVE_PATH + 'glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "        for line in f:\n",
        "            word_vector = line.split()\n",
        "            word = word_vector[0]\n",
        "            word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "            self.__glove_dict[word] = word_vector_arr\n",
        "        f.close()\n",
        "\n",
        "\n",
        "    def Restore(self):\n",
        "        \"\"\"\n",
        "        TODO: Restore train, test, and movie dataset to initial state\n",
        "        \"\"\"\n",
        "        self.train = deepcopy(self.__train)\n",
        "        self.test = deepcopy(self.__test)\n",
        "        self.movie = deepcopy(self.__movie)\n",
        "\n",
        "\n",
        "    def Movies(self, train=True) -> dict:\n",
        "        \"\"\"\n",
        "        TODO: Get movie list in dataset\n",
        "\n",
        "            :arg\n",
        "                train (bool): Target dataset, (train=True, test=False, all=None)\n",
        "            :return\n",
        "                dict: {index, MovieName}\n",
        "        \"\"\"\n",
        "        if train is None:\n",
        "            result = self.Movies()\n",
        "            result.update(self.Movies(False))\n",
        "            return result\n",
        "\n",
        "        target = None\n",
        "        if train is True:\n",
        "            target = self.train\n",
        "        elif train is False:\n",
        "            target = self.test\n",
        "\n",
        "        result = {}\n",
        "\n",
        "        if target is not None:\n",
        "            for elem in target:\n",
        "                result.update(elem['movieMentions'])\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def describe(self):\n",
        "        \"\"\"\n",
        "        TODO: Describe its datasets\n",
        "        \"\"\"\n",
        "        len1, len2 = len(self.train), len(self.test)\n",
        "        n1, n2 = 0, 0\n",
        "        m1, m2 = 0, 0\n",
        "\n",
        "        for e in self.train:\n",
        "            n1 += len(e['movieMentions'])\n",
        "            m1 += len(e['messages'])\n",
        "        for e in self.test:\n",
        "            n2 += len(e['movieMentions'])\n",
        "            m2 += len(e['messages'])\n",
        "\n",
        "        print('Brief information:\\n'\n",
        "              f'Length of train data: {len1}\\n'\n",
        "              f'Length of test data: {len2}\\n\\n'\n",
        "              'Data information:\\n'\n",
        "              f'Key parameters: {list(self.train[0].keys())}\\n'\n",
        "              f'Key parameters in Questions: {list(list(self.train[0][\"respondentQuestions\"].values())[0].keys())}\\n'\n",
        "              f'Key parameters in messages: {list(self.train[0][\"messages\"][0].keys())}\\n\\n'\n",
        "              'Context information:\\n'\n",
        "              f'Total mentioned movie number (train): {n1}\\n'\n",
        "              f'Total mentioned movie number in unique (train): {len(self.Movies())}\\n'\n",
        "              f'Total message number (train): {m1}\\n'\n",
        "              f'Total mentioned movie number (test): {n2}\\n'\n",
        "              f'Total mentioned movie number in unique (test): {len(self.Movies(False))}\\n'\n",
        "              f'Total message number (test): {m2}\\n'\n",
        "              f'Average mentioned movie numbers per conversation (train): {n1 / len1}\\n'\n",
        "              f'Average message numbers per conversation (train): {m1 / len1}\\n'\n",
        "              f'Average mentioned movie numbers per conversation (test): {n2 / len2}\\n'\n",
        "              f'Average message numbers per conversation (test): {m2 / len2}\\n\\n'\n",
        "              , end='')\n",
        "    \n",
        "\n",
        "    def preprocessing(self):\n",
        "        \"\"\"\n",
        "        TODO: Regroup train dataset into purposed structure and clean up data\n",
        "        \"\"\"\n",
        "        \n",
        "        ran = range(len(self.train))\n",
        "\n",
        "        # initialize list\n",
        "        self._global_movie_list = []\n",
        "        self._global_msg_list = []\n",
        "        self._local_movie_list = [[] for _ in ran]\n",
        "        self._local_msg_list = [[] for _ in ran]\n",
        "\n",
        "        for i, data in enumerate(self.train):\n",
        "            for msg in data['messages']:  # append line to the lists\n",
        "                self._local_msg_list[i].append(msg['text'])\n",
        "                self._global_msg_list.append(msg['text'])\n",
        "\n",
        "            # Extract movie indices\n",
        "            for idx, line in enumerate(self._local_msg_list[i]):\n",
        "                numbers = re.findall(r'@\\d+', line)  # find number keywords (ex: @12345)\n",
        "                for number in numbers:\n",
        "                    self._local_movie_list[i].append(number[1:])\n",
        "                    self._global_movie_list.append(number[1:])\n",
        "\n",
        "                    # Remove index string\n",
        "                    pos = line.index(number)\n",
        "                    line = self._local_msg_list[i][idx] = line[0: pos] + line[pos + len(number): len(line)]\n",
        "\n",
        "        # Construct dialog dataframe\n",
        "        self.dialog_df = pd.DataFrame(columns=[\"movieid\", \"dialog\"])\n",
        "\n",
        "        for lines, movies in zip(self._local_msg_list, self._local_movie_list):\n",
        "            dig = ''\n",
        "            for line in lines:  # concatenate all sentences in related message dialog\n",
        "                dig += ' ' + str(line)\n",
        "            \n",
        "            # Append dialog related to movies\n",
        "            for mv in movies:  # No such as movie: add new row\n",
        "                if self.dialog_df[self.dialog_df['movieid'] == mv].empty:\n",
        "                    newrow = pd.DataFrame({'movieid': [mv], 'dialog': [dig]}, columns=self.dialog_df.columns)\n",
        "                    self.dialog_df = pd.concat([self.dialog_df, newrow], ignore_index=True)\n",
        "                else:  # else append\n",
        "                    target = self.dialog_df[self.dialog_df['movieid'] == mv].index[0]\n",
        "                    self.dialog_df.iloc[target, 1] = self.dialog_df.iloc[target, 1] + ' ' + dig\n",
        "        \n",
        "        # Drop NaN with empty sentence\n",
        "        self.dialog_df['dialog'].dropna(how='any', inplace=True)\n",
        "\n",
        "    \n",
        "    def make_summary(self):\n",
        "        \"\"\"\n",
        "        TODO: make summary of dialog using GloVe + TextRank\n",
        "        \"\"\"\n",
        "        self.dialog_df['sentences'] = self.dialog_df['dialog'].apply(sent_tokenize)\n",
        "        \n",
        "        sentence_max = 100\n",
        "\n",
        "        def resize_sentence(sentences):\n",
        "            slice_num = len(sentences) // sentence_max\n",
        "            if len(sentences) % sentence_max:\n",
        "                slice_num += 1\n",
        "            sentences = sentences[: : slice_num]\n",
        "\n",
        "            return sentences\n",
        "        \n",
        "        self.dialog_df['sentences'] = self.dialog_df['sentences'].apply(resize_sentence)\n",
        "\n",
        "        stop_words = stopwords.words('english')\n",
        "\n",
        "        # tokenization\n",
        "        def tokenization(sentences):\n",
        "            return [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "        # Preprocessing\n",
        "        def preprocess_sentence(sentence):\n",
        "            # lower case\n",
        "            sentence = [re.sub(r'[^a-zA-z\\s]', '', word).lower() for word in sentence]\n",
        "            # remove stopwords\n",
        "            return [word for word in sentence if word not in stop_words and word]\n",
        "\n",
        "        # Do preproessing for all sentences\n",
        "        def preprocess_sentences(sentences):\n",
        "            return [preprocess_sentence(sentence) for sentence in sentences]\n",
        "\n",
        "        self.dialog_df['tokenized_sentences'] = self.dialog_df['sentences'].apply(tokenization)\n",
        "        self.dialog_df['tokenized_sentences'] = self.dialog_df['tokenized_sentences'].apply(preprocess_sentences)\n",
        "\n",
        "        # Embedding Dimension = 100 = GloVe dimension\n",
        "        embedding_dim = 100\n",
        "        zero_vector = np.zeros(embedding_dim)\n",
        "\n",
        "        # Obtain the sentence vector from the mean of words\n",
        "        def calculate_sentence_vector(sentence):\n",
        "            if len(sentence) != 0:\n",
        "                return sum([self.__glove_dict.get(word, zero_vector) for word in sentence]) / len(sentence)\n",
        "            else:\n",
        "                return zero_vector\n",
        "        \n",
        "        def sentences_to_vectors(sentences):\n",
        "            return [calculate_sentence_vector(sentence) for sentence in sentences]\n",
        "\n",
        "        # Do sentence embedding\n",
        "        self.dialog_df['SentenceEmbedding'] = self.dialog_df['tokenized_sentences'].apply(sentences_to_vectors)\n",
        "        self.dialog_df[['SentenceEmbedding']]\n",
        "\n",
        "        def similarity_matrix(sentence_embedding):\n",
        "            length = len(sentence_embedding)\n",
        "            sim_mat = np.zeros([length, length])\n",
        "\n",
        "            for i in range(length):\n",
        "                for j in range(length):\n",
        "                    sim_mat[i][j] = cosine_similarity(sentence_embedding[i].reshape(1, embedding_dim), sentence_embedding[j].reshape(1, embedding_dim))[0, 0]\n",
        "            return sim_mat\n",
        "        \n",
        "        # Get similarity matrix\n",
        "        self.dialog_df['SimMatrix'] = self.dialog_df['SentenceEmbedding'].apply(similarity_matrix)\n",
        "\n",
        "        # TextRank\n",
        "        def calculate_score(sim_matrix):\n",
        "            nx_graph = nx.from_numpy_array(sim_matrix)\n",
        "            scores = nx.pagerank_numpy(nx_graph)\n",
        "            return scores\n",
        "        \n",
        "        self.dialog_df['score'] = self.dialog_df['SimMatrix'].apply(calculate_score)\n",
        "        \n",
        "        # Write summary using TextRank score\n",
        "        def ranked_sentences(sentences, scores, n=3):\n",
        "            top_scores = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "            top_n_sentences = [sentence for score, sentence in top_scores[:n]]\n",
        "            return \" \".join(top_n_sentences)\n",
        "\n",
        "        self.dialog_df['summary'] = self.dialog_df.apply(lambda x: ranked_sentences(x.sentences, x.score), axis=1)\n",
        "    \n",
        "\n",
        "    def get_frequency_matrix(self, tags):\n",
        "        \"\"\"\n",
        "        TODO: compute the frequency of tag words to obtain the TF-IDFs matrix\n",
        "\n",
        "            :arg\n",
        "                tags (list): list of key words.\n",
        "            :return\n",
        "                pandas.DataFrame: frequency matrix of tag words.\n",
        "        \"\"\"\n",
        "        stop_word_eng = set(stopwords.words('english'))\n",
        "        ran = range(len(self.train))\n",
        "\n",
        "        msg_list = deepcopy(self._local_msg_list)\n",
        "\n",
        "        for i in ran:\n",
        "            msg_list[i] = [j for j in msg_list[i] if j not in stop_word_eng]  # Clear stopwords\n",
        "\n",
        "        # Lemmatizer class\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = RegexpTokenizer('[\\w]+')\n",
        "\n",
        "        x = pd.DataFrame(columns=['id'] + tags)\n",
        "\n",
        "        for idx, msg in enumerate(msg_list):\n",
        "            result_pre_lem = [token.tokenize(j) for j in msg]\n",
        "            middle_pre_lem = [r for j in result_pre_lem for r in j]\n",
        "            final_lem = [lemmatizer.lemmatize(j) for j in middle_pre_lem if not j in stop_word_eng]  # Remove stopword\n",
        "\n",
        "            # Lemmatization\n",
        "            english = pd.Series(final_lem)\n",
        "            for word in english:\n",
        "                if word in tags:\n",
        "                    for movie in self._local_movie_list[idx]:\n",
        "                        if x[x['id'] == movie].empty:\n",
        "                            new_row = pd.DataFrame({'id': [movie]}, columns=x.columns)\n",
        "                            x = pd.concat([x, new_row], ignore_index=True)\n",
        "                            x.fillna(0, inplace=True)\n",
        "                        x.loc[x['id'] == movie, word] += 1\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def get_tfidf_matrix(self, **tfidf_keys):\n",
        "        \"\"\"\n",
        "        TODO: Compute TF-IDFs matrix\n",
        "\n",
        "            :arg\n",
        "                tfidf_keys(keyword dict): TfidfVectorizer parameters\n",
        "            :return\n",
        "                numpy.ndrarry: TF-IDFs matrix\n",
        "                numpy.ndarray: feature name of TF-IDFs (word)\n",
        "        \"\"\"\n",
        "        # Vectorizer class\n",
        "        tfidf = TfidfVectorizer(**tfidf_keys)  # Ignore English Stopwords\n",
        "\n",
        "        # Obtain matrix\n",
        "        tfidf_df = tfidf.fit_transform(self.dialog_df['dialog'])\n",
        "\n",
        "        return tfidf_df.toarray(), tfidf.get_feature_names_out()\n",
        "\n",
        "\n",
        "    def create_evaluation_matrix(self):\n",
        "        result = []\n",
        "        for dialog in self.train:\n",
        "          for respondent in dialog['respondentQuestions']:\n",
        "            result.append([respondent,\n",
        "                        float(dialog['respondentQuestions'][respondent]['suggested']+dialog['respondentQuestions'][respondent]['seen']),\n",
        "                        float(dialog['respondentQuestions'][respondent]['seen']),\n",
        "                        float(dialog['respondentQuestions'][respondent]['suggested']+dialog['respondentQuestions'][respondent]['seen']+dialog['respondentQuestions'][respondent]['liked'])])\n",
        "          for intq in dialog['initiatorQuestions']:\n",
        "            result.append([intq,\n",
        "                       float(dialog['initiatorQuestions'][intq]['suggested']+dialog['initiatorQuestions'][intq]['seen']),\n",
        "                       float(dialog['initiatorQuestions'][intq]['seen']),\n",
        "                       float(dialog['initiatorQuestions'][intq]['suggested']+dialog['initiatorQuestions'][intq]['seen']+dialog['initiatorQuestions'][intq]['liked'])])\n",
        "        \n",
        "        test_data = pd.DataFrame(result, columns=[\"movie_id\",\"suggested\",\"seen\",\"liked\"] )\n",
        "        print(test_data.values[0])\n",
        "        grouped_data = test_data.groupby('movie_id').mean()\n",
        "        scaler = StandardScaler()\n",
        "        grouped_data = pd.DataFrame(scaler.fit_transform(grouped_data), columns = [\"suggested\",\"seen\",\"liked\"])\n",
        "        print(grouped_data.values[0])\n",
        "        reformat = []\n",
        "        for data in grouped_data.values:\n",
        "          reformat.append([data[0]*data[1], data[1], data[1]*data[2]])\n",
        "\n",
        "        test_grouped = pd.DataFrame(reformat, columns= [\"suggested\",\"seen\",\"liked\"], index = grouped_data.index)\n",
        "\n",
        "        eval_sim = cosine_similarity(test_grouped, test_grouped)\n",
        "        test_sim = pd.DataFrame(eval_sim, index = test_grouped.index, columns=test_grouped.index)\n",
        "\n",
        "        test_pred = predict_rating(test_grouped.transpose().values, test_sim.values)\n",
        "        test_pred = pd.DataFrame(test_pred, index = test_grouped.transpose().index, columns=test_grouped.transpose().columns)\n",
        "        test_data = test_pred.transpose()\n",
        "\n",
        "        return test_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSzWmuxVICMK"
      },
      "source": [
        "# Initialize\n",
        "Import dataset, describe it briefly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDNS2-WXICML",
        "outputId": "851e0196-5613-45e3-88c1-88ff03548ecd"
      },
      "outputs": [],
      "source": [
        "parser = RedialParser(BASE_PATH + 'dataset')\n",
        "parser.describe()  # Describe read dataset\n",
        "\n",
        "# Size of train data\n",
        "num = len(parser.train)\n",
        "print(f'length of train dataset: {num}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yCOY6V0ICMM"
      },
      "source": [
        "# Preprocessing\n",
        "Clear the special character and extract the text and movie indices\n",
        "- example: \"I like animations like @84779 and @191602\" â†’ [i like animations like  and ], [84779, 191602]\n",
        "\n",
        "\n",
        "Specific:\n",
        "* Transform dataset structure.\n",
        "    * Original: [movieMentions, {messages}, conversationId, ...]\n",
        "    * Transformed: [movie_indices], [message_contexts], [[1st_movie_index], [2nd_...], ...], [[1st_message_context], [2nd_...], ...]\n",
        "    * Dialog Dataframe (*self.dialog_df*): {'movie_id': '1st message' + '2nd message' + ...} - Used in generation of **TF-IDF** matrix\n",
        "* Recognize movie indices\n",
        "    * **@** recognition: use re library's *findall(@\\d+)* function, it only detects '@' + index strings.\n",
        "* Clean up meaningless values\n",
        "    * Special characters: use re library's format *\\w+*, it only receives widechar characters.\n",
        "    * Movie index: remove context of them by using text slicing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bl3KzbaXICMM",
        "outputId": "5c833684-7f64-4775-b56d-8e862d1ea72f"
      },
      "outputs": [],
      "source": [
        "parser.preprocessing()\n",
        "parser.dialog_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Add dirty data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy original\n",
        "original_df = parser.dialog_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dirty_row = pd.DataFrame({\n",
        "    'movieid': ['999995', '999996', '999997', '999998', '999999'],\n",
        "    'dialog': ['hi avenger right right hero care care thanks right hero hero hero help help pretty planning captain movie recommendations  am fiance super super super of movies hero avenger  hero i captain avenger nigh night and pretty super enjoy hero anything  pretty super might super hero super was a good pretty what s avenger super avenger great super  avenger it avenger about a baby avenger works for a company and gets  adopted it hero avenger funny avenger seems avenger amazing amazing a obsessed hero favorite pretty have hero animated  recommendations amer hero hero action captain avenger captain hero hero america like comedies hero i hero hero avenger a avenger more depth that is a tough one but i will remember  captain captain was hero good one action thanks seems cool thanks for the avenger avenger ready avenger hero if hero are hero end animated great firestick animated animated hero captain action glad  captain captain i could help nice take care hero avenger',\n",
        "    'comedy scary love animation artistic war sci blood hero romantic action recommendation happy fine animation artistic war sci blood hero romantic action comedy scary love comedy scary love war war war hero romantic action comedy hero romantic action comedy',\n",
        "    'this is a hero movie that kids like to show there is many heros with sci fi mechanism and lot of kids likes it much and also parents are liked it too I will gald to introduct that thank you for listening whatsup other recommendation is animation it has robot character that is cute all group of ages liked this movie',\n",
        "    'funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny funny',\n",
        "    'savior hero savior action savior action savior savior savior savior savior action hero savior savior savior action hero savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior savior action savior savior savior savior savior hero savior savior savior savior savior hero savior savior savior savior savior']\n",
        "})\n",
        "dirty_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser.dialog_df = pd.concat([parser.dialog_df, dirty_row], ignore_index=True)\n",
        "parser.dialog_df.tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4EJLn2eICMP"
      },
      "source": [
        "# Tokenization\n",
        "* 1. Obtain keywords (summary) using **TextRank**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser.make_summary()\n",
        "parser.dialog_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* 2. Extract words and their counts related to the movies. (Did not used, only for eye inspection.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "IiQ8YSSZICMP",
        "outputId": "785d0288-93f5-47ec-d9b9-8c72d7937fab"
      },
      "outputs": [],
      "source": [
        "# Tag words words related with movie genres\n",
        "mv_tags = ['comedy','scary','love','animation','artistic','war','sci','blood','hero','romantic','action']\n",
        "frequency = parser.get_frequency_matrix(mv_tags)\n",
        "frequency.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLWCNk0QICMQ"
      },
      "source": [
        "* 3. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "nYqovoDcICMQ",
        "outputId": "137a1b01-7a02-4402-a66a-d7901d9d5b6b"
      },
      "outputs": [],
      "source": [
        "tfidf_mat, tfidf_columns = parser.get_tfidf_matrix(stop_words='english', min_df=0.2)\n",
        "\n",
        "# Construct dataset with id + word vectors\n",
        "cdata = np.concatenate((parser.dialog_df['movieid'].to_numpy().reshape(len(parser.dialog_df['dialog']), 1), tfidf_mat), axis=1)\n",
        "df_mv_tfidf = pd.DataFrame(cdata, columns=['id'] + tfidf_columns.tolist())\n",
        "df_mv_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTB9UHGSqTY"
      },
      "source": [
        "* 4. dataframe for collaborative filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "rhBW2gN3SwPH",
        "outputId": "c3305a2d-6ba1-4627-bcf7-da205a61a21b"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_grouped = df_mv_tfidf.groupby('id').mean()\n",
        "word_sim = cosine_similarity(df_grouped, df_grouped)\n",
        "df_word_sim = pd.DataFrame(word_sim, index = df_grouped.index, columns=df_grouped.index)\n",
        "df_word_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "HMNhkS-XS6Fr",
        "outputId": "c97105e9-07f4-49d4-bbd8-76c6b93035b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "word_pred = predict_rating(df_grouped.transpose().values, df_word_sim.values)\n",
        "df_word_pred = pd.DataFrame(word_pred, index = df_grouped.transpose().index, columns=df_grouped.transpose().columns)\n",
        "collab_data = df_word_pred.transpose()\n",
        "collab_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI0M3Pd3ICMS"
      },
      "source": [
        "# Similarity Metrics\n",
        "* Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3-bC2YZICMS"
      },
      "outputs": [],
      "source": [
        "# Note: the consine similarity function's denominator has 1e-7 minimum value to avoid the divbyzero.\n",
        "c_sim = lambda X, Y: np.dot(X, Y) / (1e-7 + norm(X) * norm(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-DmfBwCICMS"
      },
      "source": [
        "Recommendation function\n",
        "* param:\n",
        "    * data: array, vector space of texts.\n",
        "    * mv: target movie's index\n",
        "    * length: maximum length of recommendation\n",
        "        * default: 5\n",
        "    * simf: similarity meterices\n",
        "        * default: cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XtMLss-ICMT"
      },
      "outputs": [],
      "source": [
        "def recommend(df, index, matrix, length=5, simf=c_sim):\n",
        "    sim = []\n",
        "\n",
        "    if df[df['movieid'] == str(index)].empty:\n",
        "        return sim\n",
        "\n",
        "    target = df[df['movieid'] == str(index)].index[0]\n",
        "\n",
        "    for idx, data in enumerate(matrix):\n",
        "        if idx != target:\n",
        "            sim.append([simf(data, matrix[target]), df.iloc[idx, 0]])\n",
        "    \n",
        "    sim.sort()\n",
        "    sim.reverse()\n",
        "    return sim[:length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "39hsgfvPTrYX",
        "outputId": "a2beff0e-51e0-4154-9984-d59b1726ad44"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(recommend(parser.dialog_df, 80067, tfidf_mat, 10), columns=['Similarity', 'Movie Index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "5M3bFSGfTsYI",
        "outputId": "8d624925-1282-4244-f1ad-7955a859ca30"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(recommend(parser.dialog_df, 80067, collab_data.values, 10), columns=['Similarity', 'Movie Index'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1sgYBfjhJv1"
      },
      "source": [
        "# Evaluation with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "JPlXg1bhhWci",
        "outputId": "e3a1625f-151f-439e-9ef4-355375ac5d73"
      },
      "outputs": [],
      "source": [
        "test_df = parser.create_evaluation_matrix()\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "EMdb5mGBtr_j",
        "outputId": "1d72e4ec-190e-4eb2-823b-3cae7af1150b"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(recommend(test_df.values,'80067',30), columns=['Similarity', 'Movie Index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdW72BsvNE_I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
